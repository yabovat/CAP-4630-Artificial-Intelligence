# -*- coding: utf-8 -*-
"""Homework_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19NcrVvWfdgbV-IXPDp2HxBOD___K00qw

Summary:
In Homework 2, I implemented logistic regression from scratch and applied it to a dataset of student exam scores. Key steps included:
Data Preparation: Loaded the dataset and visualized it using a scatter plot to differentiate passing and failing students.
Sigmoid Function: Implemented the sigmoid function to map values to probabilities.
Cost and Gradient Calculation: Developed the cross-entropy cost function and computed the gradient to measure the model's fit and update parameters.
Gradient Descent: Trained the model using gradient descent with a learning rate of 0.001 over 80,000 iterations, printing and plotting costs.
Decision Boundary: Plotted the learned decision boundary on the exam score data.
This covered the core components of logistic regression, including data visualization, model training, and decision boundary visualization.



# Logistic Regression Homework

This is the 2nd assignment for CAP 4630 and we will implement logistic regression and apply it to two
different datasets. \
You will use **"Tasks"** and **"Hints"** to finish the work. **(Total 100 Points)** \
You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras.

**Task Overview:**
- Logistic Regression

Name: Yahya Abovat

Collaboration: N/A

## 1 - Logistic Regression ##
### 1.1 Packages

Import useful packages for scientific computing and data processing.

**Tasks:**
1. Import numpy and rename it to np.
2. Import pandas and rename it to pd.
3. Import the pyplot function in the libraray of matplotlib and rename it to plt.

References:
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.

**Attention:**
1. After this renaming, you will use the new name to call functions. For example, **numpy** will become **np** in the following sections.
"""

# Task 1: Import Statments
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### 1.2 - Data Preparation ##

Prepare the data for regression task. **(20 Points)**

**Tasks:**
1. Load data for logistic regression.
2. **Generate the SCATTER PLOT of the data**.

**Hints:**
1. The data file is "data_logistic.csv", which are exam scores for students.
2. The data is organized by column: x1 (exam 1 score), x2 (exam 2 score), and label y (pass 1 or fail 0).
3. Please use different colors for postive(label=1) and negative(label=0) data.
4. An example of scatter plots is shown below.

![](https://drive.google.com/uc?export=view&id=1CPv5s4W8SkUMa_sXCIz-NejSnFj-e1IH)
"""

# Task 2: Data Preparation
from google.colab import files
uploaded = files.upload()

for filename, content in uploaded.items():
    print(f'Uploaded file "{filename}" with {len(content)} bytes')

# Load data for nonlinear regression
data_path = "data_logistic.csv"
df = pd.read_csv(data_path)

# Display the first few rows of the dataset
print(df.head())

# Load data
df = pd.read_csv('data_logistic.csv')

# Separate positive and negative examples
positive_data = df[df['label'] == 1]
negative_data = df[df['label'] == 0]

# Scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(positive_data['x1'], positive_data['x2'], color='black', label='Pass (1)', marker = '+')
plt.scatter(negative_data['x1'], negative_data['x2'], color='gold', label='Fail (0)')
plt.title('Scatter Plot of Exam Scores')
plt.xlabel('Exam 1 Score')
plt.ylabel('Exam 2 Score')
plt.legend()
plt.show()

"""### 1.3 - Sigmoid function ##


Implement sigmoid function so it can be called by the rest of your program. **(20 Points)**

**Tasks:**
1. Implement the sigmoid function (**def sigmoid(z):**).
2. Test the sigmoid function by function **plotting** with test data (X, Y) where Y = sigmoid(X).

**Hints:**
1. Given the class material, sigmoid function is defined as:
$g(z) = \frac{1}{1+e^{-z}}$.
2. You may consider X = np.linspace(-5, 5, 1000) to plot the curve.
3. Plot Y against X.
4. An example of plot for validation is shown below:

![](https://drive.google.com/uc?export=view&id=18j5oHdw78uVm2WwHsdIb4hwhpXDxR37S)
"""

# Task 3: Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

X = np.linspace(-5, 5, 1000)
Y = sigmoid(X)

plt.figure(figsize=(8, 6))
plt.plot(X, Y, label='Sigmoid Function')
plt.title('Sigmoid Function Curve')
plt.xlabel('X')
plt.ylabel('sigmoid(X)')
plt.legend()
plt.show()

"""### 1.4 - Cost function and gradient ##

Implement the cross entropy cost function and its gradient for logistic regression. **(30 Points)**

**Tasks:**
1. Implement the "cal_cost" to compute the cost.
2. Implement the "cal_grad" to compute the gradients.
3. Test "cal_cost" and "cal_grad" with initial values and print out the results.

**Hint:**
1. The cross entropy cost function (J(θ)) in logistic regression is shown below. It involves two terms, including ylog(h) and (1-y)log(1-h) where h is the function of x.

![](https://drive.google.com/uc?export=view&id=1xLhlPFI4wekwuA7lFm7ebRVt0XBZk3e7)

2. The gradient of the cost J(θ) is a vector of the same length as θ where the $j$th element (for $j = 0, 1, . . . , n)$ is defined below. You may do a hand calculation to justify the first order derivative with the term above.

![](https://drive.google.com/uc?export=view&id=1xfA0A0xyRv2L5JZIdedAmEZxZ3DwpOCF)

3. When you implement J(θ), please use eps = 1e-15 to prevent possible "divide by 0 exception" in second term. You may think about the reason.
4. You may consider the below templates for two functions:

    def cal_cost(theta, X, y):

        htheta = ...
        term1 = ...  /* matrix_multiplication(log(htheta), y)
        term2 = ...  /* matrix_multiplication(log(1-htheta+eps), (1-y))
        J = - 1 / m * (term1 + term2)

        return cost


    def cal_grad(theta, X, y):

        htheta = ...
        term1 = ... /* matrix_multiplication(transpose(X), (htheta - y))  //you may think about why transpose(x)
        grad = 1 / m * term1

        return grad
5. It involves matrix multiplication and you may consider the function of np.matmul or np.dot.

6. Initialize the intercept term (constant term) with **ones** and the theta with **zeros**. Test the functions with these initial values. \
    **Expected outputs:**\
    Cost at initial theta : 0.6931471805599445\
    Gradient at inital theta : [-0.1        -10.91242026 -11.73652937]

"""

#Task 4: Cost function and gradient
X = df[['x1', 'x2']].values
y = df['label'].values

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cal_cost(theta, X, y):
    eps = 1e-15
    m = len(y)

    htheta = sigmoid(np.dot(X, theta))

    term1 = np.dot(np.log(htheta + eps), y)
    term2 = np.dot(np.log(1 - htheta + eps), (1 - y))

    J = -1 / m * (term1 + term2)

    return J

def cal_grad(theta, X, y):
    m = len(y)

    htheta = sigmoid(np.dot(X, theta))

    term1 = np.dot(X.T, (htheta - y))

    grad = 1 / m * term1

    return grad

initial_theta = np.zeros(X.shape[1] + 1)
X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]

cost_at_initial_theta = cal_cost(initial_theta, X_with_intercept, y)
print(f"Cost at initial theta: {cost_at_initial_theta}")

grad_at_initial_theta = cal_grad(initial_theta, X_with_intercept, y)
print(f"Gradient at initial theta: {grad_at_initial_theta}")

"""## 1.5 Train parameters with Gradient Descent ##


Train parameters using Gradient Descent. **(15 Points)**

**Tasks:**
1. Calculate best fit theta by Gradient Descent with learning rate of **0.001 (1e-3)** and epoch of **80K**. The initial theta from above blocks is used as initial values.
2. Print out the best theta (the last one is considered as the best here) and its corresponding cost.
3. **Plot the decision boundary**.

**Hints:**
1. You may take gradient descent in homework 1 as an template.
2. Derive the boundary line from **sigmoid(theta[0]+ X1 * theta[1] + X2* theta[2])=0.5**. Think about why we get the line by setting **the activated probability to 0.5**. Also, try to calculate the final relationship between X1 and X2. When sigmoid(X) = 0.5, what is the value of x? Check the generated plot in 1.3.
3. The validation of first 5 epochs (updated theta and cost): \
------Epoch 0------\
Theta: [0.0001     0.01091242 0.01173653]\
Cost: 0.6996118077359638\
------Epoch 1------\
Theta: [-0.0001129   0.00053949  0.00229352]\
Cost: 0.6649331468590681\
------Epoch 2------\
Theta: [-5.93604956e-05  8.33145873e-03  1.07754324e-02]\
Cost: 0.6679914364992459\
------Epoch 3------\
Theta: [-0.0002356   0.0004607   0.00370829]\
Cost: 0.6545873034874964\
------Epoch 4------\
Theta: [-0.00020363  0.00683227  0.01065138]\
Cost: 0.6563302142684528
4. You may take the plots below as an exmample:

![](https://drive.google.com/uc?export=view&id=1xLg9LrIF888gGXj3zRAG9iJLsyAmgPQg)

5. It may take ~1 min to finish running.
"""

# Task 5: Train parameters with Gradient Descent
# Gradient Descent function
def gradient_descent(X, y, theta, learning_rate, epochs):
    m = len(y)
    costs = []

    for epoch in range(epochs):
        htheta = sigmoid(np.dot(X, theta))
        gradient = cal_grad(theta, X, y)
        theta = theta - learning_rate * gradient

        # Compute and store cost for every 1000 epochs
        if epoch % 1000 == 0:
            cost = cal_cost(theta, X, y)
            if epoch > 70980:
              print(f"------Epoch {epoch}------")
              print(f"Theta: {theta}")
              print(f"Cost: {cost}")
            costs.append(cost)

    return theta, costs

learning_rate = 1e-3
epochs = 80000

# Initialize theta with zeros
initial_theta = np.zeros(X.shape[1] + 1)
X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]

best_theta, costs = gradient_descent(X_with_intercept, y, initial_theta, learning_rate, epochs)

# Print the best theta and its corresponding cost
print("\nBest Theta:", best_theta)
print("Cost at Best Theta:", cal_cost(best_theta, X_with_intercept, y))

# Plot the decision boundary
plt.figure(figsize=(8, 6))
plt.scatter(positive_data['x1'], positive_data['x2'], color='black', label='Pass (1)', marker = '+')
plt.scatter(negative_data['x1'], negative_data['x2'], color='gold', label='Fail (0)')

boundary_x = np.array([X[:, 0].min(), X[:, 0].max()])
boundary_y = (-1 / best_theta[2]) * (best_theta[1] * boundary_x + best_theta[0])

plt.plot(boundary_x, boundary_y, color='blue', label='Decision Boundary')

plt.title('Decision Boundary')
plt.xlabel('Exam 1 Score')
plt.ylabel('Exam 2 Score')
plt.legend()
plt.show()

"""
### 1.6 Evaluating Logistic Regression

Evaluate the model with given data. **(15 Points)**

**Tasks:**
1. Calculate the training accuracy and **PRINT IT OUT**.
2. Evaluate the predicted probability of the learnt model with x1 = 56 and x2 = 32 and **PRINT IT OUT**.


**Hints:**
1. Positive(prediction>0.5) and negative(prediction<=0.5).
2. The prediction results are based on acceptance probability. Given the two exam scores, we expected the model yields either high probability of "fail" or low probability of "pass".
3. Training accuracy should be around **85%**."""

# Function to predict labels and probabilities based on learned parameters
def predict_probabilities(theta, X):
    probabilities = sigmoid(np.dot(X, theta))
    predictions = (probabilities > 0.5).astype(int)
    return predictions, probabilities

# Calculate training accuracy
predictions, _ = predict_probabilities(best_theta, X_with_intercept)
accuracy = np.mean(predictions == y) * 100
print(f"Training Accuracy: {accuracy:.2f}%")

# Evaluate the predicted probabilities for x1 = 56 and x2 = 32
new_data = np.array([[56, 32]])
new_data_with_intercept = np.c_[np.ones((1, 1)), new_data]
predictions_new_data, probabilities_new_data = predict_probabilities(best_theta, new_data_with_intercept)

print(f"Predicted Probability for x1 = 56, x2 = 32 (Negative): {probabilities_new_data[0]:.4f}")
print(f"Predicted Probability for x1 = 56, x2 = 32 (Positive): {1 - probabilities_new_data[0]:.4f}")
